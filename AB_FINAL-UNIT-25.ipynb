{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0beda7ae",
   "metadata": {},
   "source": [
    "# **Inicio del AB FINAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747274e6",
   "metadata": {},
   "source": [
    "## **Preparacion de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0dee50",
   "metadata": {},
   "source": [
    "En este caso, se ha decidido crear este dataframe unificando todos los CSV en un único DataFrame y añadir una columna que identifique el origen (por ejemplo, source_id). Ya que la otra es crear un diccionario con cada csv separado, pero la custion es que las estructuras de columnas son muy parecidas y no paraece que se vaya a necesitar tratamientos específicos por fichero, si no mas bien en conjunto.\n",
    "\n",
    "Por ello, se considera que es mas optima esta opción de unificar todos los datos en el mismo Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d3307",
   "metadata": {},
   "source": [
    "### Ruta de los *csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16595b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias necesarias\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"pandas\",\n",
    "    \"numpy\", \n",
    "    \"openpyxl\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"scipy\",\n",
    "    \"cmdstanpy\",\n",
    "    \"pystan\",\n",
    "    \"prophet\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"✓ {package} ya está instalado\")\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {package}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "            print(f\"✓ {package} instalado correctamente\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"⚠️ Error instalando {package}, continuando...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b59fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69287bb1",
   "metadata": {},
   "source": [
    "### **Función de ingerta de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992c480",
   "metadata": {},
   "source": [
    "En este caso, tras invesigar, encontre la funcion ***glob***, la cual busca un patron en comun al cual comparar en una ruta especificada y a partir de ahi, extrae todas las rutas cojn dicho patron. Por ello, para insertar todos los csv, hare una función que busque archivos acabados en \"*.csv*\".\n",
    "\n",
    "Además, haré un bucle con un for para recorrer y comprobar que se hayan descargado y leido los datos correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030cf32",
   "metadata": {},
   "source": [
    "#### ***Merge de los csv***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abef26c",
   "metadata": {},
   "source": [
    "Una vez ya nos hemos asegurado de que todos los csv estan preparados para juntarlos en el mismo Dataframe, toca juntarlos propiamente en el mismo Dataframe para trabajar a posteriori con ellos.\n",
    "\n",
    "Además, como vimos en el resultado de la celda anterior, hay muchas columnas llamadas Unnamed, las cuales no tienen nada de información dentro, por lo que a la hora de crear el Dataframe, lo solucionare eliminando todas estas columnas que no necesitamos y solo ensucian el Dataframe.\n",
    "\n",
    "\n",
    "\n",
    ">  # Resultados de la celda anterior\n",
    "    AguaEmbalsada_RioCofio_LaAcena.csv\n",
    "    Columnas:['anio', 'mes', 'hec_cub', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13']\n",
    "    ---------------------------------------------------------------\n",
    "    Archivo: AguaEmbalsada_RioGuadalix_Pedrezuela.csv\n",
    "    Columnas: ['anio', 'mes', 'hec_cub', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13']quote\n",
    "\n",
    "\n",
    "\n",
    "Además, para la nueva columna para el Dataframe, he decidido que el contenido de dicha columna, en vez de ser la ruta de donde se han importados los datos, sea mejor directamente el nombre del embalse de donde se han sacado los datos. Esto, lo he hecho mediante la siguiente forma:\n",
    "\n",
    "\n",
    "\n",
    ">   # Añadir columna con el nombre del archivo:\n",
    "    nombre = Path(archivo).stem\n",
    "    embalse = nombre.split(\"_\")[-1]          \n",
    "    df[\"embalse\"] = embalse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354585ed",
   "metadata": {},
   "source": [
    "#### **Comparacion de los csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2a86c",
   "metadata": {},
   "source": [
    "Una vez que ya se que se han leido todos los csv correctamente, quiero comparar las columnas de los distintos *csv*, ya que para poder fusionar en un mismo Dataframe todos los datos, necesitamos asegurarnos de que se van a apilar correectamente y no se van a generear columnbas indeseadas por que ciertas columnas se llamen de forma distinta, como por ejemplo:  \n",
    "\n",
    "*   La columna se llama ***Año***\n",
    "*   La columna se llama ***Fecha***\n",
    "\n",
    "Además, he estado teniendo un mismo error a la hora de leer los datos, ya que me salia que:\n",
    "\n",
    "\n",
    "    UnicodeDecodeError                        Traceback (most recent call last)\n",
    "    /tmp/ipython-input-2309794888.py in <cell line: 0>()\n",
    "          5\n",
    "          6 for archivo in CSVs:\n",
    "    ----> 7   df_temp = pd.read_csv(archivo, nrows=0, sep=\";\")\n",
    "          8   columnas_por_archivo[archivo.split(\"/\")[-1]] = list(df_temp.columns)\n",
    "          9\n",
    "\n",
    "    5 frames\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader._get_header()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader._check_tokenize_status()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.raise_parser_error()\n",
    "\n",
    "    /usr/lib/python3.12/codecs.py in decode(self, input, final)\n",
    "\n",
    "    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 8059: invalid continuation byte *texto en cursiva*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Esto indica que alguno de los CSV no está en *UTF-8*, sino probablemente en *Latin-1 (ISO-8859-1)*, cosa que al parecer es muy comun en ficheros Españoles, por lo que he metido un segundo intento de lectura con ***encoding=\"latin-1\"*** si falla con ***UTF-8.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04519a7e",
   "metadata": {},
   "source": [
    "#### **Función de Ingesta**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dbb9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta = \"data/\"\n",
    "archivos = sorted(glob.glob(ruta + \"*.csv\"))\n",
    "print(f\"Archivos encontrados: {len(archivos)}\")\n",
    "for archivo in archivos:\n",
    "    print(f\"  - {Path(archivo).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2221e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_df_embalses(archivos):\n",
    "    \"\"\"\n",
    "    Lee archivos csv desde la carpeta 'data/', los limpia y los merge en un único DataFrame.\n",
    "    \n",
    "    Pasos:\n",
    "    1) Inspecciona y compara columnas\n",
    "    2) Lee, limpia y mergea en df_embalses\n",
    "    \n",
    "    Devuelve df_embalses.\n",
    "    \"\"\"\n",
    "\n",
    "    # -------- Inspeccionar columnas --------\n",
    "    columnas_por_archivo = {}\n",
    "\n",
    "    for archivo in archivos:\n",
    "        try:\n",
    "            df_temp = pd.read_csv(archivo, nrows=0, sep=\";\", encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            df_temp = pd.read_csv(archivo, nrows=0, sep=\";\", encoding=\"latin-1\")\n",
    "        \n",
    "        columnas_por_archivo[Path(archivo).name] = list(df_temp.columns)\n",
    "\n",
    "    # Mostrar columnas de cada archivo\n",
    "    for nombre, columnas in columnas_por_archivo.items():\n",
    "        print(\"\\nArchivo:\", nombre)\n",
    "        print(\"Columnas:\", columnas)\n",
    "\n",
    "    # Comparar todas las columnas con las del primer archivo\n",
    "    referencia = list(columnas_por_archivo.values())[0]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Comparación con la referencia:\")\n",
    "    print(\"Archivo de referencia:\", list(columnas_por_archivo.keys())[0])\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for nombre, columnas in columnas_por_archivo.items():\n",
    "        if columnas == referencia:\n",
    "            print(\"✅\", nombre, \"tiene las mismas columnas\")\n",
    "        else:\n",
    "            print(\"⚠️\", nombre, \"tiene columnas diferentes\")\n",
    "            print(\"   Columnas diferentes:\", set(columnas).symmetric_difference(set(referencia)))\n",
    "\n",
    "    # -------- Leer, limpiar y mergear --------\n",
    "    lista_df = []\n",
    "    columnas_referencia = None\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Iniciando merge de {len(archivos)} archivos...\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    for archivo in archivos:\n",
    "        try:\n",
    "            df = pd.read_csv(archivo, sep=\";\", encoding=\"utf-8\")\n",
    "        except UnicodeDecodeError:\n",
    "            df = pd.read_csv(archivo, sep=\";\", encoding=\"latin-1\")\n",
    "\n",
    "        # Eliminar columnas basura tipo \"Unnamed: X\"\n",
    "        df = df.drop(columns=[col for col in df.columns if \"Unnamed\" in col])\n",
    "\n",
    "        # Añadir columna con el nombre del embalse\n",
    "        nombre = Path(archivo).stem\n",
    "        embalse = nombre.split(\"_\")[-1]\n",
    "        df[\"embalse\"] = embalse\n",
    "\n",
    "        # Registrar columnas de referencia del primer archivo\n",
    "        if columnas_referencia is None:\n",
    "            columnas_referencia = list(df.columns)\n",
    "            print(\"Columnas de referencia:\", columnas_referencia)\n",
    "\n",
    "        lista_df.append(df)\n",
    "        print(f\"✓ Leído: {Path(archivo).name}\")\n",
    "\n",
    "    # Concatenar todo\n",
    "    df_embalses = pd.concat(lista_df, ignore_index=True, sort=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Merge completado.\")\n",
    "    print(f\"Filas totales: {len(df_embalses)}\")\n",
    "    print(f\"Columnas totales: {len(df_embalses.columns)}\")\n",
    "    print(f\"Columnas finales: {list(df_embalses.columns)}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    return df_embalses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41590a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(archivos) == 0:\n",
    "    print(\"⚠️ No hay archivos .xlsx en la carpeta 'data/'\")\n",
    "    print(\"Por favor, coloca tus archivos Excel en: data/\")\n",
    "    df_embalses = None\n",
    "else:\n",
    "    df_embalses = cargar_df_embalses(archivos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65fe66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_embalses is not None:\n",
    "    df_embalses\n",
    "else:\n",
    "    print(\"Esperando a que carguen los datos...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_embalses is not None:\n",
    "    # De esta forma, usando el iloc, podemos ver si realmente estan mezclados bien los datos\n",
    "    print(df_embalses.iloc[1500:2003] if len(df_embalses) > 2003 else df_embalses.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c793a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embalses.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8b444a",
   "metadata": {},
   "source": [
    "## **Limpieza de la base de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0cd20",
   "metadata": {},
   "source": [
    "### Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081a947",
   "metadata": {},
   "source": [
    "Una cosa a tener en cuenta, es que para este trabajo, hemos pensado usar el algoritmo de prediccion de serie temporales Prophet, por lo cual la base de datos deberá estar preferentemente para la facilidad del entrenamiento del algortimo en un estilo, en el que los embalses sea las columnas y sus datos se rellenen con las cantidades de agua medidas en cada mes y año."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bb5343",
   "metadata": {},
   "source": [
    "### Limpieza técnica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68c6daf",
   "metadata": {},
   "source": [
    "#### Normalización de los meses con diccionario\n",
    "\n",
    "Ahora, ya que debe estar en formato numerico, ya lo quiera pasar mas adelante a un tipo de dato Datetype, o no, el modelo lee solamente numeros, por lo cual, debo pasarlo de los meses ***\"enero\", \"febrero\", etc...*** a los nombres de los meses a formato numérico ***\"01\", \"02\"***..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c35e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diccionario de meses → número con padding de dos dígitos\n",
    "mes_map = {\n",
    "    \"enero\": \"01\",\n",
    "    \"febrero\": \"02\",\n",
    "    \"marzo\": \"03\",\n",
    "    \"abril\": \"04\",\n",
    "    \"mayo\": \"05\",\n",
    "    \"junio\": \"06\",\n",
    "    \"julio\": \"07\",\n",
    "    \"agosto\": \"08\",\n",
    "    \"septiembre\": \"09\",\n",
    "    \"setiembre\": \"09\",   # por seguridad\n",
    "    \"octubre\": \"10\",\n",
    "    \"noviembre\": \"11\",\n",
    "    \"diciembre\": \"12\"\n",
    "}\n",
    "\n",
    "# Normalizamos y reemplazamos directamente la columna 'mes'\n",
    "df_embalses[\"mes\"] = (\n",
    "    df_embalses[\"mes\"]\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    "    .str.lower()\n",
    "    .map(mes_map)\n",
    ")\n",
    "\n",
    "# Comprobación rápida\n",
    "df_embalses[\"mes\"].head(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bffd508",
   "metadata": {},
   "source": [
    "#### Limpieza de columnas inecesarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b44156d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
