{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0beda7ae",
   "metadata": {},
   "source": [
    "# **Inicio del AB FINAL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747274e6",
   "metadata": {},
   "source": [
    "## **Preparacion de los datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0dee50",
   "metadata": {},
   "source": [
    "En este caso, se ha decidido crear este dataframe unificando todos los CSV en un único DataFrame y añadir una columna que identifique el origen (por ejemplo, source_id). Ya que la otra es crear un diccionario con cada csv separado, pero la custion es que las estructuras de columnas son muy parecidas y no paraece que se vaya a necesitar tratamientos específicos por fichero, si no mas bien en conjunto.\n",
    "\n",
    "Por ello, se considera que es mas optima esta opción de unificar todos los datos en el mismo Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d3307",
   "metadata": {},
   "source": [
    "### Ruta de los *csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16595b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias necesarias\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "packages = [\n",
    "    \"pandas\",\n",
    "    \"numpy\", \n",
    "    \"openpyxl\",\n",
    "    \"matplotlib\",\n",
    "    \"seaborn\",\n",
    "    \"scikit-learn\",\n",
    "    \"scipy\",\n",
    "    \"cmdstanpy\",\n",
    "    \"pystan\",\n",
    "    \"prophet\"\n",
    "]\n",
    "\n",
    "for package in packages:\n",
    "    try:\n",
    "        __import__(package.replace(\"-\", \"_\"))\n",
    "        print(f\"✓ {package} ya está instalado\")\n",
    "    except ImportError:\n",
    "        print(f\"Instalando {package}...\")\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "            print(f\"✓ {package} instalado correctamente\")\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(f\"⚠️ Error instalando {package}, continuando...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b59fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69287bb1",
   "metadata": {},
   "source": [
    "### **Función de ingerta de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2992c480",
   "metadata": {},
   "source": [
    "En este caso, tras invesigar, encontre la funcion ***glob***, la cual busca un patron en comun al cual comparar en una ruta especificada y a partir de ahi, extrae todas las rutas cojn dicho patron. Por ello, para insertar todos los csv, hare una función que busque archivos acabados en \"*.csv*\".\n",
    "\n",
    "Además, haré un bucle con un for para recorrer y comprobar que se hayan descargado y leido los datos correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e030cf32",
   "metadata": {},
   "source": [
    "#### ***Merge de los csv***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abef26c",
   "metadata": {},
   "source": [
    "Una vez ya nos hemos asegurado de que todos los csv estan preparados para juntarlos en el mismo Dataframe, toca juntarlos propiamente en el mismo Dataframe para trabajar a posteriori con ellos.\n",
    "\n",
    "Además, como vimos en el resultado de la celda anterior, hay muchas columnas llamadas Unnamed, las cuales no tienen nada de información dentro, por lo que a la hora de crear el Dataframe, lo solucionare eliminando todas estas columnas que no necesitamos y solo ensucian el Dataframe.\n",
    "\n",
    "\n",
    "\n",
    ">  # Resultados de la celda anterior\n",
    "    AguaEmbalsada_RioCofio_LaAcena.csv\n",
    "    Columnas:['anio', 'mes', 'hec_cub', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13']\n",
    "    ---------------------------------------------------------------\n",
    "    Archivo: AguaEmbalsada_RioGuadalix_Pedrezuela.csv\n",
    "    Columnas: ['anio', 'mes', 'hec_cub', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13']quote\n",
    "\n",
    "\n",
    "\n",
    "Además, para la nueva columna para el Dataframe, he decidido que el contenido de dicha columna, en vez de ser la ruta de donde se han importados los datos, sea mejor directamente el nombre del embalse de donde se han sacado los datos. Esto, lo he hecho mediante la siguiente forma:\n",
    "\n",
    "\n",
    "\n",
    ">   # Añadir columna con el nombre del archivo:\n",
    "    nombre = Path(archivo).stem\n",
    "    embalse = nombre.split(\"_\")[-1]          \n",
    "    df[\"embalse\"] = embalse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354585ed",
   "metadata": {},
   "source": [
    "#### **Comparacion de los csv**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b2a86c",
   "metadata": {},
   "source": [
    "Una vez que ya se que se han leido todos los csv correctamente, quiero comparar las columnas de los distintos *csv*, ya que para poder fusionar en un mismo Dataframe todos los datos, necesitamos asegurarnos de que se van a apilar correectamente y no se van a generear columnbas indeseadas por que ciertas columnas se llamen de forma distinta, como por ejemplo:  \n",
    "\n",
    "*   La columna se llama ***Año***\n",
    "*   La columna se llama ***Fecha***\n",
    "\n",
    "Además, he estado teniendo un mismo error a la hora de leer los datos, ya que me salia que:\n",
    "\n",
    "\n",
    "    UnicodeDecodeError                        Traceback (most recent call last)\n",
    "    /tmp/ipython-input-2309794888.py in <cell line: 0>()\n",
    "          5\n",
    "          6 for archivo in CSVs:\n",
    "    ----> 7   df_temp = pd.read_csv(archivo, nrows=0, sep=\";\")\n",
    "          8   columnas_por_archivo[archivo.split(\"/\")[-1]] = list(df_temp.columns)\n",
    "          9\n",
    "\n",
    "    5 frames\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader.__cinit__()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader._get_header()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.TextReader._check_tokenize_status()\n",
    "\n",
    "    parsers.pyx in pandas._libs.parsers.raise_parser_error()\n",
    "\n",
    "    /usr/lib/python3.12/codecs.py in decode(self, input, final)\n",
    "\n",
    "    UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf1 in position 8059: invalid continuation byte *texto en cursiva*\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Esto indica que alguno de los CSV no está en *UTF-8*, sino probablemente en *Latin-1 (ISO-8859-1)*, cosa que al parecer es muy comun en ficheros Españoles, por lo que he metido un segundo intento de lectura con ***encoding=\"latin-1\"*** si falla con ***UTF-8.***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04519a7e",
   "metadata": {},
   "source": [
    "#### **Función de Ingesta**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
